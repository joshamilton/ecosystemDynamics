{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation and Time-Series Analysis\n",
    "\n",
    "This notebook aims to provide an overview of my efforts to develop better optimization routines for time-series analysis of microbial community abundance data. The notebook includes a general overview of the project along with key results.\n",
    "\n",
    "__Hypothesis__: being \"smart\" about optimization choices can lead to better parameter estimates using real ecological datasets, relative to using \"off-the-shelf\" optimization routines built into Python and/or R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "Time-series are all over microbial ecology. Unfortunately, real data are messy and often don't conform to the requirements of parameter estimation software. Some examples include:\n",
    "\n",
    "* Unevenly spaced time-points, when parameter estimation routines assume data are evenly spaced\n",
    "\n",
    "* Missing environmental data, when parameter estimation routines assume environmental data were measured along with microbial observations.\n",
    "\n",
    "* Seasonal time-series, or time-series with other long gaps, when parameter estimation routines assume data were recorded without long gaps.\n",
    "\n",
    "Examples of ecological datasets subject to one or more of these problems include:\n",
    "\n",
    "* NTL-MO - multiple years of data, but even spacing and long gaps (courtesy of long Wisconsin winters)\n",
    "\n",
    "* Marine time-series (HOT, SPOT, BATS) - multiple years of data, but even spacing\n",
    "\n",
    "* Human microbiome time-series - research this, try to find some that are disease-related. Schloss C. diff datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Workflow\n",
    "\n",
    "I propose to develop a custom optimization routine tailored to multi-year ecological data series with unevenly-spaced time-points and long gaps. The routine will estimate microbe-microbe and microbe-environment (\"ecological\") parameters from data, and I will (hopefully) demonstrate my routine performs better than off-the-shelf software.\n",
    "\n",
    "My routine will have two features:\n",
    "\n",
    "1. Rather than performing interpolation before the parameter estimation, I will perform interpolation simultaneously with the estimation. This allows the values of missing data points to emerge naturally from the model rather than assuming a fixed structure for the missing data (linear interpolation, etc). This also has the advantage of capturing dynamics that don't conform to the standard interpolation functions (such as blooms and other non-monotonic behavior).\n",
    "\n",
    "2. For datasets with large gaps (such as winter), rather than interpolating across the long gaps, I will perform a single optimization over multiple time-series (of one year each) in parallel, rather than over a single time-series spanning the entire length of the dataset.\n",
    "\n",
    "The performance of my routine will be benchmarked against routines which perform interpolation across a single time-series spanning the length of the dataset.\n",
    "\n",
    "Initial work will be performed on the __Trout Bog epilimnion 16S rRNA time series__, which currently has two years of replicated data (2007, 2008) and six more years being sequenced (2009, 2012-2016).\n",
    "\n",
    "| Lake / Layer | Year | Replicated Samples |\n",
    "|--------------|------|--------------------|\n",
    "| TBE          | 2007 | 38                 | \n",
    "| TBE          | 2008 | 26                 | \n",
    "| TBE          | 2009 | 14                 | \n",
    "| TBE          | 2012 | 14                 | \n",
    "| TBE          | 2013 | 11                 | \n",
    "| TBE          | 2014 | 8                  | \n",
    "| TBE          | 2015 | 10                 | \n",
    "| TBE          | 2016 | 9                  | \n",
    "\n",
    "However, I'm worried about the sparsity of the Trout Bog datasets. Samples are taken once or twice a week, meaning many of the data values still need to be interpolated. I'm worried that having too few observations relative to the length of the data won't provide enough information for the least-squares estimation. I'm considering starting with one of the dense human microbiome datasets and iteratively removing samples to see if performance degrades at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTU Selection\n",
    "\n",
    "I have chosen to use [unique sequence variants](http://www.nature.com/ismej/journal/vaop/ncurrent/full/ismej2017119a.html) (of 16s rRNA genes) as my definition of a microbial species. There are a number of algorithms to identify USVs, I chose to use [deblur](https://github.com/biocore/deblur) due to our previous work with Rob Knight's group. Because Alex was unsure of the exact parameters used for her paper, I decided to redownload and re-deblur the data. Bonus, now I have a pipeline for when the new samples come in. Deblurring resulted in __71,915 OTUs__.\n",
    "\n",
    "Furthermore, Robin has observed that some of the full-length reads are low quality, so it's worth thinking about more extensive QC when new samples come in.\n",
    "\n",
    "![Qual Scores of TB epi samples](images/tbe-qual-scores.png)\n",
    "\n",
    "To reduce this number, I chose to retain OTUs that had an __average relative abundance__ (across two samples per time point) in some fraction of samples (__persistence__). OTUs below this threshold were lumped into an \"other OTU\". The plot below shows the number of fraction of sequences retained and total number of OTUs for a bunch of parameter values.\n",
    "\n",
    "![Persistence and Abundance Filtering](images/filter-persist-abund.png)\n",
    "\n",
    "Values of 0.001 relative abundance and 20% persistence resulted in __72 OTUs, 90% total reads, and 50% of sequences in all samples__. However, in some samples the \"other OTU\" contained almost 50% of the sequences from the sample. Upon inspection, this \"other OTU\" was dominated by three sequences with > 0.01 relative abundance.\n",
    "\n",
    "Thus I added a third criterion to the filter: I chose to retain OTUs that had a __relative abundance__ in some fraction of samples (__persistence__) __OR__ were abundant in at least one sample at some relative abundance (__bloom__).\n",
    "\n",
    "Sadly I have no 3D plot of these criteria. But criteria of 0.001 in at least 15% of samples OR an abundance of 0.01 in at least one sample resulted in 76 OTUs. Other parameter choices are possible, but these values retained ~90% of sequences in each sample while minimizing the number of total OTUs.\n",
    "\n",
    "### Final Parameters\n",
    "Rel. abund. and persistence - 0.001 relative abundance in at least 15% of samples  \n",
    "Bloom - 0.01 relative abundance  \n",
    "Total OTUs: 137\n",
    "\n",
    "__Note__ for Cristina: Mean-variance scaling plots and Breuschâ€“Pagan tests of heteroskedasticity suggest a relative abundance cutoff between 0.001 and 0.003 to be outside the realm of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "I intend to develop a (possibly multivariate) [auto-regressive model](https://en.wikipedia.org/wiki/Autoregressive_model) (AR) to predict time-series data. I selected AR modeling due to its long use in ecology, and the fact that it is stochastic. Due to the long time periods between observations (days to weeks) and the short time scale of microbial behavior (hours to days), I don't believe that a mechanistic model makes much sense.\n",
    "\n",
    "I will be following the [Box-Jenkins method](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method) to select the appropriate AR model. This method applies (a possibly simplified) autoregressive integrated moving average (ARIMA) model to find the best fit of model to a time-series.\n",
    "\n",
    "ARIMA models contain three components. The auto-regressive (AR) term indicates that the variable is a function of its own prior values. The integrated (I) term indicates that the variables have been replaced with their differences (difference between current and previous values). This differencing may be necessary to make the time-series stationary (see below). The moving average (MA) term indicates that the error term is linear combination of errors from the current and previous times. ARIMA models are denoted __ARIMA(p, d, q)__, where __p__ is the number of time lags in the auto-regressive model, __d__ is the number of differences in the integrated model, and __q__ is the number of time lags in the moving average model.\n",
    "\n",
    "ARIMA models can also be generalized as <a href='http://onlinelibrary.wiley.com/wol1/doi/10.1890/0012-9615(2003)073[0301:ECSAEI]2.0.CO;2/abstract'>multi-variate AR models</a>, in which the abundance of an OTU is not only a function its own (previous) abundance, but also of other variables (such as other OTUs or environmental variables).\n",
    "\n",
    "In this study, I propose to develop and assess three distinct models for time-series abundance of OTUs:\n",
    "\n",
    "* Univariate auto-regressive model - OTU only depends on its previous value(s)\n",
    "\n",
    "* Multivariate auto-regressive model, only species-species interactions - OTU depends in its previous value(s) and previous value(s) of other OTUs\n",
    "\n",
    "And possibly, \n",
    "\n",
    "* Multivariate auto-regressive model with species-species and species-environment interactions - OTU depends in its previous value(s) and previous value(s) of other OTUs and environmental data\n",
    "\n",
    "* Multivariate auto-regressive model with species-environment interactions - OTU depends in its previous value(s) and previous value(s) of environmental data\n",
    "\n",
    "In any case, the first step is to determine the appropriate values of __p__, __d__, and __q__ for the ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Single OTU ARIMA Model\n",
    "\n",
    "The first model I plan to develop is a univariate auto-regressive model, in which the abundance of an OTU depends only on its previous abundance(s) plus an error term.\n",
    "\n",
    "### Time-Series Stationarity\n",
    "\n",
    "The __I__ term in an ARIMA indicates the number of differencing terms required to make sure the time series is stationary. A time-series is [stationary](https://en.wikipedia.org/wiki/Stationary_process) means that the mean and variance don't change with time. A time-series which is non-stationary must be transformed to become stationary. The most common causes of non-stationarity is a trend in the mean, which can be caused by a [unit root](https://en.wikipedia.org/wiki/Unit_root) or a [deterministic trend](https://en.wikipedia.org/wiki/Trend_stationary). If a process has a unit root, it must be differenced to become stationariy. If a process has a trend, it must be de-trended. Thus, our first task is to determine if our time-series are stationary or not. If not, we must make them stationary.\n",
    "\n",
    "Stationarity can be assessed many ways including histograms of abundance data for each OTU (should be normally distributed) and via statistical testing, such as the [Mann-Kendall test](http://www.statisticshowto.com/mann-kendall-trend-test/), the [ADF test](http://www.statisticshowto.com/adf-augmented-dickey-fuller-test/), and the [KPSS test](http://www.statisticshowto.com/kpss-test/).\n",
    "\n",
    "These tests can be summarized:\n",
    "\n",
    "| Test | Null Hypothesis | Rejection Indicates | \n",
    "|--------------|------|--------------------| \n",
    "| Mann-Kendall | No trend | Trend |\n",
    "| ADF-Mean | Non-stationarity due to a unit root | Stationary | \n",
    "| ADF-Trend | Non-stationarity due to a trend | Trend-stationary | \n",
    "| KPSS-Mean | Stationary | Non-stationarity due to a unit root\n",
    "| KPSS-Trend | Trend-stationary | Non-stationarity due to a unit root\n",
    "\n",
    "#### Dealing with Zeros\n",
    "\n",
    "By their nature, relative abundance data are non-stationary so they must be transformed. I opted for a natural logarithm transformation. Of course, zero values cannot be log-transformed. Based on mean-variance scaling, relative abundances below 0.001 are indistinguishable from noise. Thus, after filtering, I replaced all relative abundances below 0.001 with a small number (0.00065) before log-transforming the data. A value of 0.00065 was selected based on [empirical studies](http://onlinelibrary.wiley.com/doi/10.1002/9781119976462.ch4/summary) of dealing with zeros. \n",
    "\n",
    "#### Statistical Tests of Stationarity\n",
    "\n",
    "Representative time-series and histograms of ln(relative abundance) are shown below. As you can see, the time-series do not appear to be stationary.\n",
    "\n",
    "![Representative Log-transformed Time-Series](images/log-transformed-time-series.png)\n",
    "\n",
    "Statistical testing confirms this. Because the statistical tests require evenly-spaced time-series, a perfomed the tests using a variety of interpolation methods. Results were generally consistent and are shown for the linear case. For all 152 time-series (76 OTUs x 2 years), the results of the statistical tests were:\n",
    "\n",
    "| Test | Outcome | # Time-series\n",
    "|--------------|------|--------------------| \n",
    "| Mann-Kendall | Reject null, detect a trend | 127\n",
    "| ADF-Mean | Reject null, stationary | 7\n",
    "| ADF-Trend | Reject null, trend-stationary | 9\n",
    "| KPSS-Mean | Reject null, non-stationary due to a unit root | 91\n",
    "| KPSS-Trend | Reject null, non-stationary due to a unit root | 73\n",
    "\n",
    "Overall, these results suggest the data are non-stationary, and this may arise due a trend (M-K test) or a unit root (KPSS test).\n",
    "\n",
    "I then took the first-difference of the time-series. For a time-series, the first difference of $Y_t$ is $Y_t - Y_{t-1}$. I performed first-differencing using a number of interpolation schemes. Plots of representative time-series and histograms of ln(relative abundance) are shown below. As you can see, the time-series appear to be stationary.\n",
    "\n",
    "![Representative Log-transformed Time-Series](images/log-transformed-diff-time-series.png)\n",
    "\n",
    "Statistical testing confirms this. Because the statistical tests require evenly-spaced time-series, a perfomed the tests using a variety of interpolation methods. Results were generally consistent and are shown for the linear case. For all 152 time-series (76 OTUs x 2 years), the results of the statistical tests were:\n",
    "\n",
    "| Test | Outcome | # Time-series\n",
    "|--------------|------|--------------------| \n",
    "| Mann-Kendall | Reject null, detect a trend | 55\n",
    "| ADF-Mean | Reject null, stationary | 111\n",
    "| ADF-Trend | Reject null, trend-stationary | 78\n",
    "| KPSS-Mean | Reject null, non-stationary due to a unit root | 3\n",
    "| KPSS-Trend | Reject null, non-stationary due to a unit root | 9\n",
    "\n",
    "These results suggest that the first-differenced time-series are trend-stationary (non-stationary due to a deterministic trend in the mean). The next step is to detrend the time-series. Furthermore, the results indicate that the ARIMA process has __d__ = 1.\n",
    "\n",
    "### Detrending\n",
    "\n",
    "The statistical tests indicate many of the OTUs exhibit a trend in their abundance. This is borne out in plots of (log-transformed) relative abundance.\n",
    "\n",
    "Some OTUs have trends...\n",
    "\n",
    "![Time-Series with Trends](images/time-series-trends.png)\n",
    "\n",
    "while others do not.\n",
    "\n",
    "![Time-Series without Trends](images/time-series-no-trends.png)\n",
    "\n",
    "I speculate that these trends are seasonal. When plotting two years of data (2007 and 2008), seasonality is observed for some taxa.\n",
    "\n",
    "![Recurring Seasonal Trends](images/annual-trends.png)\n",
    "\n",
    "Statistical tests again detect a trend, but with less confidence I suspect that this is because the time-series starts in spring of one season and ends during fall of another, so the seasonal trend is reflected in the data. Hopefully the support for a long-term trend will go away once additional years of data are added.\n",
    "\n",
    "Because de-trending is intended to remove long-term trends in the data, not periodic variability, I have opted not to de-trend the data at this time.\n",
    "\n",
    "### Auto-Regressive and Moving-Average Components\n",
    "\n",
    "After a time-series has been made stationary, the next step is to determine the order of the auto-regressive and moving-average components. The [auto-regressive component](https://en.wikipedia.org/wiki/Autoregressive_model) of the model describes the dependency of the current value on its previous value(s), plus a stochastic term. In contrast, in a [moving-average model](https://en.wikipedia.org/wiki/Moving-average_model), the current value depends on the previous value(s) of a stochastic term. In both cases, the number of previous values is referred to as the __order__.\n",
    "\n",
    "If an auto-regresive model is appropriate, the time-series should show strong auto-correlations. That is, the current value of the time-series should be correlated to previous values. This is precisely what I see in the data (representative time-series with linear interpolation, blue curves represent 95% confidence interval for an uncorrelated process):\n",
    "\n",
    "![Representative auto-correlation functions](images/autocorr.png)\n",
    "\n",
    "Other interpolation methods give similar results. Thus, we have an auto-regressive model, with no moving average component (__q__=0). The __order__ of the auto-regressive component can be found from partial auto-correlation plots, which gives the correlation which cannot be accounted for by previous lags. Thus, the lag at which the partial correlation function becomes zero gives the order of the AR process.\n",
    "\n",
    "![Representative partial auto-correlation functions](images/pautocorr.png)\n",
    "\n",
    "These results suggest the time-series is auto-regressive with order 1 (__p__=1).\n",
    "\n",
    "### Summary of the AR Model\n",
    "\n",
    "I will be fitting an ARIMA(1, 1, 0) model to the Trout Bog epilimnion time-series. What does the model look like?\n",
    "\n",
    "A 1st order AR model has the form:\n",
    "\n",
    "$y_t = c + \\phi y_{t-1} + \\epsilon_t$ \n",
    "\n",
    "where $t$ is time, $y$ is the time-series value, $c$ is a constant, $\\phi$ is the autoregression parameter, and $\\epsilon_t$ is the error term.\n",
    "\n",
    "Our model will need to be first-differenced. That is, values of $y$ are replaced with their first differences, $y'$: \n",
    "\n",
    "$y'_t = y_t - y_{t-1}$\n",
    "\n",
    "Performing a substitution and rearranging, we obtain:\n",
    "\n",
    "$y_t = c + y_{t-1} + \\phi(y_{t-1} + y_{t-2}) + \\epsilon_t$\n",
    "\n",
    "Note that some $y_t$ are known from data, and the others are parameters to be identified via the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Optimization\n",
    "\n",
    "As described above, my parameter estimation routine will have two features:\n",
    "\n",
    "1. Rather than performing interpolation before the parameter estimation, I will perform interpolation simultaneously with the estimation. This allows the values of missing data points to emerge naturally from the model rather than assuming a fixed structure for the missing data (linear interpolation, etc). This also has the advantage of capturing dynamics that don't conform to the standard interpolation functions (such as blooms and other non-monotonic behavior).\n",
    "\n",
    "2. For datasets with large gaps (such as winter), rather than interpolating across the long gaps, I will perform a single optimization over multiple time-series (of one year each) in parallel, rather than over a single time-series spanning the entire length of the dataset. The difference between these two formulations is shown conceptually below:\n",
    "\n",
    "![Single vs multiple time-series](images/two-vs-one-time-series.png)\n",
    "\n",
    "The performance of my routine will be benchmarked against routines which perform interpolation across a single time-series spanning the length of the dataset.\n",
    "\n",
    "Parameters will be estimated using least squares estimation (as opposed to maximum likelihood). Apart from personal familiarity, I don't have a good justification for this choice and am open to changing my mind.\n",
    "\n",
    "### Model Formulation for the Single OTU AR Model\n",
    "\n",
    "For a single time-series, the nonlinear least squares formulation is:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\text{min} && \\sum_{t \\in T_{obs}} (y_t - y_{t,obs}) \\\\\n",
    "& \\text{s.t} && y_t = c + y_{t-1} + \\phi(y_{t-1} + y_{t-2}) + \\epsilon_t && t \\in T_{obs} \n",
    "\\end{aligned}\n",
    "\n",
    "where $t$ is time, $y$ is the time-series value, $c$ is a constant, $\\phi$ is the autoregression parameter, and $\\epsilon_t$ is the error term. $T_{obs}$ is the set of time-points for which data were observed, and $y_{t,obs}$ is the observed value at time $t$. Thus, the least squares estimation only considers those time-points for which we have observed data.\n",
    "\n",
    "This formulation is a non-linear program (NLP), which can be solved via freely-available, commercial-grade solvers like [IPOPT](https://projects.coin-or.org/Ipopt). NLPs require an initial parameter estimation to obtain a good solution, I'm still thinking about a heuristic for obtaining a good starting point.\n",
    "\n",
    "For multiple time-series, the formulation is as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\text{min} && \\sum_{l \\in L} \\sum_{t \\in T_{obs}} (y_t - y_{t,obs}) \\\\\n",
    "& \\text{s.t} && y_{l,t} = c + y_{l,t-1} + \\phi(y_{l,t-1} + y_{l,t-2}) + \\epsilon_{l,t} && l \\in L && t \\in T_{obs}  \n",
    "\\end{aligned}\n",
    "\n",
    "where $l$ is the year and $L$ is the set of all years for which data are avaiable. Note that in both cases, there is still a single set of parameters (c, $\\phi$) to be estimated. \n",
    "\n",
    "These optimization problems will be solved for each OTU independently. In later formulations, I will introduce vector auto-regression, in which I solve for interactions between OTUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Extended Model: Multivariate ARIMA Model\n",
    "\n",
    "The second model I plan to develop is a multivariate auto-regressive model, in which the abundance of an OTU depends on its previous abundance as well as the previous abundance of other OTUs.\n",
    "\n",
    "If there __n__ interacting species, then the multivariate ARIMA(1,1,0) model (VARIMA(1,1,0)) looks as follows:\n",
    "\n",
    "$Y_t = C + Y_{t-1} + \\Phi(Y_{t-1} + Y_{t-2}) + E_t$\n",
    "\n",
    "where $Y_t$ is an $n \\times 1$ vector of relative abundances, $C$ is a $n \\times 1$ vector of constants, $\\Phi$ is an $n \\times n$ matrix whose elements $\\phi_{i,j}$ give the effect of the abundance of species $j$ on the growth rate of species $i$, and $E_t$ is a $n \\times 1$ vector of errors.\n",
    "\n",
    "For a single time-series, the nonlinear least squares formulation is:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\text{min} && \\sum_{n} \\sum_{t \\in T_{obs}} (y_{n,t} - y_{n,t_{obs}}) \\\\\n",
    "& \\text{s.t} && Y_t = C + Y_{t-1} + \\Phi(Y_{t-1} + Y_{t-2}) + E_t && t \\in T_{obs} \n",
    "\\end{aligned}\n",
    "\n",
    "This formulation is also non-linear program (NLP) and will be solved in a similar way as the uni-variate model. For multiple time-series, the formulation is as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\text{min} && \\sum_{l \\in L} \\sum_{n} \\sum_{t \\in T_{obs}} (y_{n,t} - y_{n,t_{obs}}) \\\\\n",
    "& \\text{s.t} && Y_{l,t} = C + Y_{l,t-1} + \\Phi(Y_{l,t-1} + Y_{l,t-2}) + E_{l,t} && l \\in L && t \\in T_{obs} \n",
    "\\end{aligned}\n",
    "\n",
    "where $l$ is the year and $L$ is the set of all years for which data are avaiable. Note that in both cases, there is still a single set of parameters (C, $\\Phi$) to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Extended Model: ARIMA Model with Environmental Terms\n",
    "\n",
    "The third model I plan to develop is a univariate auto-regressive model, in which the abundance of an OTU depends on its previous abundance(s), the value of environmental parameters (covariates), and an error term. The parameter optimation problem is:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\text{min} && \\sum_{l \\in L} \\sum_{t \\in T_{obs}} (y_t - y_{t,obs}) \\\\\n",
    "& \\text{s.t} && y_{l,t} = c + y_{l,t-1} + \\alpha u_t + \\phi(y_{l,t-1} + y_{l,t-2}) + \\epsilon_{l,t} && l \\in L && t \\in T_{obs}  \n",
    "\\end{aligned}\n",
    "\n",
    "where $t$ is time, $y$ is the time-series value, $c$ is a constant, $\\phi$ is the autoregression parameter, and $\\epsilon_t$ is the error term. $u$ is a $q \\times 1$ vector containing values of environmental variables, and $\\alpha$ is an $1 \\times q$ vector giving the strength of each environmental variable on the OTU abundance. Again, $l$ is the year and $L$ is the set of all years for which data are available. The parameters to be estimated are (c, $\\alpha$, $\\phi$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Extended Model: VARIMA Model with Environmental Terms\n",
    "\n",
    "The final model I plan to develop is a multivariate auto-regressive model, in which the abundance of an OTU depends on its previous abundance(s), the value of environmental parameters (covariates), and an error term. The problem generalizes in a similar way:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\text{min} && \\sum_{l \\in L} \\sum_{n} \\sum_{t \\in T_{obs}} (y_{n,t} - y_{n,t_{obs}}) \\\\\n",
    "& \\text{s.t} && Y_{l,t} = C + Y_{l,t-1} + AU_t + \\Phi(y_{l,t-1} + Y_{l,t-2}) + E_{l,t} && l \\in L && t \\in T_{obs}  \n",
    "\\end{aligned}\n",
    "\n",
    "where $t$ is time, $Y_t$ is an $n \\times 1$ vector of relative abundances, $C$ is a $n \\times 1$ vector of constants,\n",
    "$U$ is a $q \\times 1$ vector containing values of environmental variables, and A is an $n \\times q$ matrix whose elements $\\alpha_{i,j}$ give the strength of each environmental variable on the OTU abundance. $\\Phi$ is an $n \\times n$ matrix whose elements $\\phi_{i,j}$ give the effect of the abundance of species $j$ on the growth rate of species $i$, and $E_t$ is a $n \\times 1$ vector of errors. The parameters to be estimated are (C, A, and $\\Phi$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "\n",
    "The final section will focus on validating the models, including obtaining confidence intervals for model parameters and performing cross-validation to test model accuracy. \n",
    "\n",
    "### Confidence Intervals\n",
    "The confidence interval of a parameter is the range of values (the interval) that acts as a good estimate of the unknown parameter. A good optimization routine should result in narrow confidence intervals for most parameters, and parameters with narrow intervals are suggestive of ecological interactions. I anticipate that not all parameters will have good (narrow) confidence intervals. Parameters with poor (wide) either cannot be determined from the data, or are unimportant in the model. I am presently undecided about the best way to obtain confidence intervals. For multi-variate models, parameters include interaction coefficients between taxa and the environment, and may point to ecological interactions among taxa.\n",
    "\n",
    "### Cross-validation\n",
    "Cross-validation is a technique for assessing how well a model generalizes to data it hasn't seen before.\n",
    "\n",
    "I propose to cross-validate my models using leave-p-out validation on time-series. That is, I will remove 'p' years of data and build the model (the training set), and assess the model's ability to predict the remaining years of data (the validaion set). Accuracy will be evaluated using mean squared error. By performing the validation for different values of 'p', I will also understand how many years of data are required for robust parameter estimation.\n",
    "\n",
    "Leave-p-out validation is \"exhaustive,\" in that it assesses all possible ways to partition the data into testing and training sets. If my optimization routine proves computationally expensive, I will perform non-exhaustive cross-validation.\n",
    "\n",
    "It __may__ be possible to perform leave-p-out validation on individual samples as well. While this is unlikely to be feasible given the large number of datapoints, the results will suggest the number of samples required for robust parameter estimation.\n",
    "\n",
    "### Comparing Multiple Models\n",
    "I anticipate that the more complex, multivariate models will provide a better fit to the data, as described by the residual squared error. This value is the objective function for the parameter estimation problem so is easily obtainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bogtags]",
   "language": "python",
   "name": "conda-env-bogtags-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
